{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b97ad47-de8a-49fc-bfb7-83d6c74eb648",
   "metadata": {},
   "source": [
    "<h3>Clustring</h3>\n",
    "\n",
    "\n",
    "   <li>1. Clusters are collections of similar data </li> \n",
    "   <li>2. Clustering is a type of <b>unsupervised learning technique </b> where the algorithm groups data points that are similar to each other </li> \n",
    "   <li>3. The Correlation Coefficient describes the strength of a relationship. </li> \n",
    "\n",
    "### Common clustering algorithms\n",
    "\n",
    "* **K-Means**: Probably the most popular. You choose the number of clusters *k*, and the algorithm partitions the data into *k* groups based on distance to cluster centers.\n",
    "* **Hierarchical Clustering**: Builds a tree (dendrogram) of clusters, either bottom-up (agglomerative) or top-down (divisive).\n",
    "* **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: Finds clusters based on dense regions of points. It’s good for irregularly shaped clusters and handling outliers.\n",
    "* **Gaussian Mixture Models (GMMs)**: Assumes data points are generated from a mixture of Gaussian distributions, which allows for soft clustering (a point can belong to multiple clusters with probabilities).\n",
    "\n",
    "### Applications\n",
    "\n",
    "* Customer segmentation in marketing\n",
    "* Image segmentation\n",
    "* Document/topic grouping\n",
    "* Anomaly detection\n",
    "* Recommender systems\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b30c5d0-16de-4c4b-a9ad-3f6163f0cd45",
   "metadata": {},
   "source": [
    "<h2>1. K-Means Clustering</h2>\n",
    "\n",
    "**Idea**: Split the data into *k* groups where each point belongs to the group with the nearest “center” (centroid).\n",
    "\n",
    "**How it works**:\n",
    "\n",
    "1. Pick the number of clusters (*k*).\n",
    "2. Randomly place *k* centroids.\n",
    "3. Assign each point to the nearest centroid.\n",
    "4. Move the centroids to the average (mean) of all points in their group.\n",
    "5. Repeat steps 3–4 until the centroids stop moving much (convergence).\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "* Simple, fast, works well with spherical clusters.\n",
    "* Scales to large datasets.\n",
    "\n",
    "**Cons**:\n",
    "\n",
    "* You must choose *k* ahead of time.\n",
    "* Doesn’t handle irregular shapes or outliers well.\n",
    "* Sensitive to initial placement of centroids.\n",
    "\n",
    "**Example use**: Customer segmentation (grouping customers by buying habits).\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Hierarchical Clustering**\n",
    "\n",
    "**Idea**: Build a tree (dendrogram) of clusters that shows how points group together at different levels of similarity.\n",
    "\n",
    "Two main types:\n",
    "\n",
    "* **Agglomerative (bottom-up)**: Start with each point as its own cluster, then repeatedly merge the two closest clusters until everything is in one big cluster.\n",
    "* **Divisive (top-down)**: Start with all points in one cluster and split them recursively.\n",
    "\n",
    "**How it works (Agglomerative example)**:\n",
    "\n",
    "1. Treat each point as a cluster.\n",
    "2. Calculate distances between all clusters (single-link, complete-link, or average-link are common ways).\n",
    "3. Merge the two closest clusters.\n",
    "4. Repeat until only one cluster remains.\n",
    "5. The dendrogram lets you “cut” the tree at a chosen level to decide the number of clusters.\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "* No need to choose *k* up front (though you still decide where to “cut” the dendrogram).\n",
    "* Good for understanding relationships between clusters.\n",
    "\n",
    "**Cons**:\n",
    "\n",
    "* Computationally expensive for large datasets.\n",
    "* Sensitive to noise/outliers.\n",
    "* Once two clusters merge, you can’t undo it.\n",
    "\n",
    "**Example use**: Gene expression analysis in biology (to see how genes or samples group together).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f0702ac-2864-42a2-bf99-b5820d8bac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c32abb8-2f94-4af1-8e3a-1d7376c9a37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "beerc = pd.read_csv(\"C:\\\\Users\\\\SAKSHI BADOLA\\\\Documents\\\\MachineLearning\\\\DataSets\\\\beerc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79156fd8-9346-498c-91cd-e68f1bf575fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
